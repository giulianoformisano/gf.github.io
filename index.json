
    
    
    
    [{"authors":null,"categories":null,"content":"I am a 3rd year PhD student in the Department of Statistics at the University of Oxford, under the supervision of Patrick Rebeschini and George Deligiannidis. I am funded by EPSRC.\nMy research interests include reinforcement learning, optimization and learning theory. In particular, I focus on building and analysing reinforcement learning algorithms using standard optimization tools, such as natural gradient descent and mirror descent.\nDownload my CV. ","date":1664496000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1664496000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a 3rd year PhD student in the Department of Statistics at the University of Oxford, under the supervision of Patrick Rebeschini and George Deligiannidis. I am funded by EPSRC.","tags":null,"title":"Carlo Alfano","type":"authors"},{"authors":["Carlo Alfano","Patrick Rebeschini"],"categories":null,"content":"","date":1664496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664496000,"objectID":"cc2d0d105c2d2dbd60328e36ba512da5","permalink":"https://c-alfano.github.io/publication/paper-2/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/publication/paper-2/","section":"publication","summary":"We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q-value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy class.","tags":null,"title":"Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization","type":"publication"},{"authors":["Carlo Alfano"],"categories":null,"content":"","date":1663768800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1663768800,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://c-alfano.github.io/talk/linear-convergence-for-natural-policy-gradient-with-log-linear-policy-parametrization/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/linear-convergence-for-natural-policy-gradient-with-log-linear-policy-parametrization/","section":"event","summary":"We analyze the convergence rate of the unregularized natural policy gradient algorithm with log-linear policy parametrizations in infinite-horizon discounted Markov decision processes. In the deterministic case, when the Q-value is known and can be approximated by a linear combination of a known feature function up to a bias error, we show that a geometrically-increasing step size yields a linear convergence rate towards an optimal policy. We then consider the sample-based case, when the best representation of the Q- value function among linear combinations of a known feature function is known up to an estimation error. In this setting, we show that the algorithm enjoys the same linear guarantees as in the deterministic case up to an error term that depends on the estimation error, the bias error, and the condition number of the feature covariance matrix. Our results build upon the general framework of policy mirror descent and extend previous findings for the softmax tabular parametrization to the log-linear policy","tags":[],"title":"Linear Convergence for Natural Policy Gradient with Log-linear Policy Parametrization","type":"event"},{"authors":["Carlo Alfano","Patrick Rebeschini"],"categories":null,"content":"","date":1632355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"9c180c49ce06dc4a6248c7cd268b05ed","permalink":"https://c-alfano.github.io/publication/paper-1/","publishdate":"2021-09-23T00:00:00Z","relpermalink":"/publication/paper-1/","section":"publication","summary":"Cooperative multi-agent reinforcement learning is a decentralized paradigm in sequential decision making where agents distributed over a network iteratively collaborate with neighbors to maximize global (network-wide) notions of rewards. Exact computations typically involve a complexity that scales exponentially with the number of agents. To address this curse of dimensionality, we design a scalable algorithm based on the Natural Policy Gradient framework that uses local information and only requires agents to communicate with neighbors within a certain range. Under standard assumptions on the spatial decay of correlations for the transition dynamics of the underlying Markov process and the localized learning policy, we show that our algorithm converges to the globally optimal policy with a dimension-free statistical and computational complexity, incurring a localization error that does not depend on the number of agents and converges to zero exponentially fast as a function of the range of communication.","tags":null,"title":"Dimension-Free Rates for Natural Policy Gradient in Multi-Agent Reinforcement Learning","type":"publication"}]